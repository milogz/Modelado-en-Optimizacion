{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42bd1cc-d2d1-4997-8e88-2b6a13d35470",
   "metadata": {},
   "source": [
    "# Modelado en Optimización (IIND-2501)\n",
    "## Módulo 3 - Tipos de problemas de optimización y principales estrategias de solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41819d17-47e9-46d8-a2c3-d62eabe84734",
   "metadata": {},
   "source": [
    "El foco de la primera parte del curso estuvo en cómo **modelar problemas de decisión** y resolverlos a través de la optimización, acudiendo a herramientas como PuLP (Python) o Excel-Solver. Este módulo se enfoca en **conocer el proceso de solución** de un problema de optimización, introduciendo las estrategias de solución comunes para diferentes tipos de problemas (lineales, no lineales, continuos, enteros) y en diferentes contextos. Específicamente, conoceremos:\n",
    "- *qué hacen internamente los solvers* cuando resolvemos problemas de optimización de diferentes tipos, y\n",
    "- *cómo las herramientas de estadística y machine learning usan la optimización* para ajustar \"modelos que aprenden\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb73324-05af-4d33-9ee2-00353bcb74aa",
   "metadata": {},
   "source": [
    "### Conceptos básicos\n",
    "\n",
    "En general, llamaremos $\\mathbf{x}$ al **vector de variables de decisión** de un problema de optimización dado. El proceso de optimización consiste en encontrar los valores de $\\mathbf{x}$ que cumplen con las restricciones establecidas y ofrecen la mejor función objetivo posible. Dependiendo del tipo de problema, existen métodos basados en derivadas, en álgebra lineal, o en procedimientos algorítmicos, que definen reglas para avanzar en la búsqueda de la mejor solución. Independientemente del método, la optimización se basa en un proceso iterativo de búsqueda que parte de una solución inicial $\\mathbf{x}^{(t=0)}$ y actualiza los valores del vector $\\mathbf{x}$ a lo largo de iteraciones ($t=1,2,...,T_{max}$) hasta encontrar la mejor solución o agotar un máximo de iteraciones. En cada iteración, se hace un movimiento partiendo del vector de decisión actual $\\mathbf{x}^{(t)}$, y \"avanzando\" una longitud $\\alpha^{(t)}$ en la dirección $\\mathbf{\\Delta x}^{(t)}$, las cuales conducen a un nuevo vector de decisiones $\\mathbf{x}^{(t+1)}$.\n",
    "\n",
    "$$\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{\\Delta x}^{(t)}$$\n",
    "\n",
    "La forma de avanzar de un $\\mathbf{x}^{(t)}$ al siguiente (i.e., la dirección y longitud de movimiento) depende del tipo de método, que a su vez depende del tipo de problema. En algunos casos, estos movimientos están basados en ecuaciones vectoriales mientras que en otros se hacen mediante reglas algorítmicas generales o acordes al problema. En este módulo, conoceremos estrategias de búsqueda de la mejor solución para problemas de optimización en diferentes contextos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c37e5-7a41-424a-b485-93f2c5ba6f6a",
   "metadata": {},
   "source": [
    "### Estructura del módulo\n",
    "\n",
    "Durante este módulo, conoceremos estrategias de solución para problemas de optimización comunes en la práctica: desde encontrar el mínimo de una función, hasta resolver problemas de decisión continuos y discretos, en casos lineales y no lineales. Específicamente, nos concentraremos en los siguientes tres bloques.\n",
    "\n",
    "1. **Optimización sin restricciones** (Semana 9): Contrastaremos procedimientos basados en vecindarios (búsqueda local) y basados en derivadas (búsqueda de gradiente) para optimizar funciones no lineales sin restricciones, introduciendo nociones de *óptimo local/global*, *incumbente*, y *convergencia* (**Lección 3.1**). Estudiaremos el uso del *descenso de gradiente* para la optimización en problemas de regresión, introduciendo la noción de *función de pérdida* como base para los problemas de estimación en *machine learning* e inteligencia artificial (**Lección 3.2**).\n",
    "   \n",
    "2. **Optimización lineal con restricciones** (Semanas 10 y 11): Retomaremos problemas de decisión como los cubiertos en la primera parte del curso, e introduciremos conceptos de *programación lineal* (y método Simplex) como estrategia vigente y base para análisis económico y métodos avanzados (**Lecciones 3.3, 3.4, y 3.5**). Como complemento (opcional), tendremos una lección introductoria que extiende las nociones al caso de la optimización no lineal (**Lección 3.6**), presentando problemas comunes y herramientas de solución disponibles.\n",
    "   \n",
    "3. **Optimización en problemas de decisiones discretas** (Semana 12): Exploraremos los retos asociados a la solución de problemas con variables enteras o binarias, presentando estrategias aproximadas (heurísticas) como alternativa práctica de solución para problemas difíciles de resolver con métodos exactos. (**Lecciones 3.7 y 3.8**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a8949-1b4a-4424-98b8-0668a8455397",
   "metadata": {},
   "source": [
    "### Objetivos de aprendizaje\n",
    "\n",
    "Como complemento al contenido previo sobre modelamiento, este módulo ofrece una introducción a los procedimientos de solución de problemas de optimización, como elemento clave bajo lineamientos ABET e INFORMS. Al finalizar, el estudiante estará en capacidad de:\n",
    "\n",
    "- Describir el **esquema general de procedimientos de búsqueda en optimización**, incluyendo búsqueda local y métodos basados en gradientes, aplicados a funciones de costo o pérdida en contextos operacionales y de analítica de datos (ABET 1: identificar y resolver problemas; INFORMS: comprensión de métodos de solución).\n",
    "- Reconocer las **estructuras algebraicas fundamentales de problemas de decisión con restricciones**, tales como regiones factibles, variables de decisión, restricciones lineales y funciones objetivo, y relacionarlas con el comportamiento del algoritmo Simplex (ABET 1: formular problemas; INFORMS: fundamentos de programación lineal).\n",
    "- Identificar **problemas que requieren formulaciones no lineales, y conocer herramientas computacionales básicas para su solución**, reconociendo que estos métodos se basan en principios análogos a los de la programación lineal (ABET 1: aplicar principios matemáticos; INFORMS: nociones de programación no lineal).\n",
    "- Diferenciar la **complejidad adicional que implica la optimización entera, y explorar alternativas aproximadas como heurísticas simples**, reconociendo sus ventajas y limitaciones en contextos prácticos (ABET 2: diseño de soluciones factibles; INFORMS: modelamiento con variables discretas y heurísticas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f7e35-c3ed-4e2b-98f7-ca6494850ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
