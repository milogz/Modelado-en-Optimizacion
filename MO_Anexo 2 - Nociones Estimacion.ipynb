{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7074b001-6156-4824-bf68-ed82bab5ff9a",
   "metadata": {},
   "source": [
    "# Anexo 2: Estimación y Regresión Lineal\n",
    "\n",
    "Este anexo complementa el notebook de **Estimación**, ofreciendo un panorama más formal e intuitivo de la regresión lineal, el método de **Mínimos Cuadrados Ordinarios (OLS)** y las principales métricas de evaluación. El propósito es servir de guía introductoria para estudiantes monitores y apoyar el tránsito hacia textos más completos como Greene, Montgomery & Runger, o Hastie et al.\n",
    "\n",
    "## 1. Panorama general\n",
    "\n",
    "La **estimación estadística** busca inferir relaciones entre variables a partir de datos observados. En particular, la **regresión lineal** intenta responder a preguntas como:\n",
    "\n",
    "- ¿Cómo se relaciona una variable respuesta \\(y\\) con factores explicativos \\(x\\)?\n",
    "- ¿Qué tanto de la variabilidad en \\(y\\) puede explicarse por un modelo lineal?\n",
    "- ¿Qué tan confiable es la predicción fuera de la muestra?\n",
    "\n",
    "En su forma más simple, una regresión lineal ajusta:\n",
    "\n",
    "$$\n",
    "y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "donde \\(\\epsilon_i\\) es el error no explicado por el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dfcc5-9b77-4ae7-8511-caaf334bcbbf",
   "metadata": {},
   "source": [
    "## 2. El método de Mínimos Cuadrados Ordinarios (OLS)\n",
    "\n",
    "La idea fundamental de OLS es elegir los parámetros \\(\\beta\\) que **minimizan la suma de cuadrados de los errores**:\n",
    "\n",
    "$$\n",
    "\\min_\\beta \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Esta formulación se inspira en la geometría: se busca la “recta” (o hiperplano) que más cerca pase de todos los puntos en promedio.  \n",
    "\n",
    "- En **Geometría Lineal** (Montgomery & Runger), el ajuste equivale a proyectar el vector de respuestas \\(y\\) sobre el subespacio generado por las variables \\(X\\).  \n",
    "- En **Econometría** (Greene), se resalta que OLS es un **estimador insesgado** y consistente bajo supuestos clásicos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafec565-2db8-4e3a-8546-63b0c45a04d7",
   "metadata": {},
   "source": [
    "### El gradiente como dirección de mejora\n",
    "\n",
    "La forma más directa de encontrar el *mejor modelo de estimación* es encontrar los coeficientes ($\\beta$) que minimizan la \"función de pérdida\"; en nuestro caso, la suma de errores cuadrados que indica que tan lejos están las estimaciones de los valores observados. Minimizar esta función corresponde a un problema de optimización sin restricciones. Una forma de encontrar la mejor solución consiste en moverse en el espacio de decisiones siguiendo la dirección en la que la función objetivo disminuye más rápidamente. El **gradiente** $\\nabla f(\\mathbf{x})$ es el vector de derivadas parciales que apunta hacia el **mayor incremento** de $f$. Para minimizar, se avanza en la dirección opuesta:\n",
    "\n",
    "$$\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\alpha^{(t)} \\nabla f(\\mathbf{x}^{(t)})$$\n",
    "\n",
    "donde $\\alpha^{(t)}$ es la *tasa de aprendizaje* o *longitud de paso*. Este procedimiento, conocido como **descenso de gradiente**, se ilustra en la **lección 3.2** y constituye la base de la mayoría de los algoritmos de aprendizaje automático modernos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cfde8-9a8d-4e4b-98ef-d686d27444ab",
   "metadata": {},
   "source": [
    "### Optimización y aprendizaje supervisado\n",
    "\n",
    "En los métodos de aprendizaje supervisado, como la regresión lineal o logística, el objetivo es ajustar un modelo parametrizado $f(\\mathbf{x}; \\boldsymbol{\\theta})$ a datos observado. \n",
    "El proceso de entrenamiento consiste en minimizar una **función de pérdida** que mide el error de predicción$$\n",
    "\n",
    "$$\\min_{\\boldsymbol{\\theta}} \\; \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\ell\\big(y_i, f(\\mathbf{x}_i; \\boldsymbol{\\theta})\\big),$$\n",
    "\n",
    "donde $\\ell(\\cdot)$ es una métrica de error (por ejemplo, el error cuadrático medio). Así, el aprendizaje es un caso particular de **optimización sin restricciones**, en el que los parámetros del modelo juegan el papel de variables de decisión y la función de pérdida reemplaza la función objetivo tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5052513-dba8-4535-89af-f94cec79ca0c",
   "metadata": {},
   "source": [
    "## 3. Supuestos clásicos de OLS\n",
    "\n",
    "Aunque muchas veces se aplican sin verificarlos, es útil reconocerlos:\n",
    "\n",
    "1. **Linealidad en parámetros**: el modelo es lineal respecto a \\(\\beta\\).  \n",
    "2. **Exogeneidad**: \\(E[\\epsilon|X] = 0\\).  \n",
    "3. **Varianza constante de errores (homocedasticidad)**.  \n",
    "4. **No colinealidad perfecta** entre los regresores.  \n",
    "5. **Distribución normal de los errores** (útil para inferencia).  \n",
    "\n",
    "Estos supuestos permiten derivar propiedades óptimas del estimador y justificación de pruebas de hipótesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c771a-6b89-4f1a-af59-4adc7538b9b2",
   "metadata": {},
   "source": [
    "## 4. Métricas de evaluación\n",
    "\n",
    "Un modelo no solo debe ajustarse, sino evaluarse:\n",
    "\n",
    "- **R² (Coeficiente de determinación):** mide proporción de variabilidad explicada.  \n",
    "- **Error Cuadrático Medio (MSE / RMSE):** magnitud típica del error en unidades de \\(y\\).  \n",
    "- **Error Absoluto Medio (MAE):** magnitud media sin penalizar cuadráticamente.  \n",
    "- **Validación cruzada:** divide los datos en entrenamiento/prueba, fundamental para evitar sobreajuste.  \n",
    "\n",
    "*(Ver Hastie, Tibshirani & Friedman, Cap. 3, para una visión moderna de validación y métricas).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1b1e6-8de6-4758-bcab-d4e5e0365107",
   "metadata": {},
   "source": [
    "## 5. Más allá de OLS\n",
    "\n",
    "- En aplicaciones modernas, OLS es la base de técnicas más avanzadas (regresión ridge, lasso, modelos no lineales).  \n",
    "- El entendimiento profundo de sus supuestos y métricas es clave para interpretar correctamente cualquier resultado predictivo.\n",
    "\n",
    "### Referencias clave\n",
    "\n",
    "- Greene, W. H. (2012). *Econometric Analysis*. Pearson. (Cap. 2, fundamentos de OLS).  \n",
    "- Montgomery, D. C., & Runger, G. C. (2014). *Applied Statistics and Probability for Engineers*. Wiley. (Cap. 11, regresión simple y múltiple).  \n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer. (Cap. 3, regresión y predicción).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a1a89-2c39-4dba-a14a-6aafe7a1b9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
